{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d791a70a-f2f7-443d-b64e-bb1ae0b7b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cd9741-d812-4aa0-8072-71704da0539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import model_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbff9e-2145-4441-8791-a9000941391a",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623c53bb-08b9-4d9b-8e57-358401187a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/12 10:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0d342-0828-4655-bf71-a702bc2d9886",
   "metadata": {},
   "source": [
    "## set up config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240f521a-b87b-4f15-8003-999852fc9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_date_str = \"2024-01-01\"\n",
    "model_name = \"credit_model_2024_09_01.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b21e01-b85d-4423-bdbe-491a2be4fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_artefact_filepath': 'model_bank/credit_model_2024_09_01.pkl',\n",
      " 'model_bank_directory': 'model_bank/',\n",
      " 'model_name': 'credit_model_2024_09_01.pkl',\n",
      " 'snapshot_date': datetime.datetime(2024, 1, 1, 0, 0),\n",
      " 'snapshot_date_str': '2024-01-01'}\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config[\"snapshot_date_str\"] = snapshot_date_str\n",
    "config[\"snapshot_date\"] = datetime.strptime(config[\"snapshot_date_str\"], \"%Y-%m-%d\")\n",
    "config[\"model_name\"] = model_name\n",
    "config[\"model_bank_directory\"] = \"model_bank/\"\n",
    "config[\"model_artefact_filepath\"] = config[\"model_bank_directory\"] + config[\"model_name\"]\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa380d-97c1-40ee-8c98-9ceb7bdb24bf",
   "metadata": {},
   "source": [
    "## load model artefact from model bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47aa9391-40e2-4676-a5d9-890f7b02da88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully! model_bank/credit_model_2024_09_01.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the pickle file\n",
    "with open(config[\"model_artefact_filepath\"], 'rb') as file:\n",
    "    model_artefact = pickle.load(file)\n",
    "\n",
    "print(\"Model loaded successfully! \" + config[\"model_artefact_filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7362cd4-0dc5-4555-843d-02806b161750",
   "metadata": {},
   "source": [
    "## load feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56346248-03ed-4be4-a092-7756c07ecc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to feature store\n",
    "folder_path_1 = \"datamart/gold/feature_store/eng/\"\n",
    "folder_path_2 = \"datamart/gold/feature_store/cust_fin_risk/\"\n",
    "files_list_1 = [folder_path_1+os.path.basename(f) for f in glob.glob(os.path.join(folder_path_1, '*'))]\n",
    "files_list_2 = [folder_path_2+os.path.basename(f) for f in glob.glob(os.path.join(folder_path_2, '*'))]\n",
    "feature_store_sdf_1 = spark.read.option(\"header\", \"true\").parquet(*files_list_1)\n",
    "feature_store_sdf_2 = spark.read.option(\"header\", \"true\").parquet(*files_list_2)\n",
    "print(\"row_count:\",feature_store_sdf_1.count())\n",
    "print(\"row_count:\",feature_store_sdf_2.count())\n",
    "\n",
    "print(\"======Feature Table 1======\")\n",
    "feature_store_sdf_1.show()\n",
    "print(\"======Feature Table 2======\")\n",
    "feature_store_sdf_2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
